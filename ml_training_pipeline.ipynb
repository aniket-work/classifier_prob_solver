{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ML Training Pipeline for Heart Disease Classification: A Dataset Exploration\n",
    "\n",
    "This is part2 of our series [Let's do ML season1 part 1](https://www.linkedin.com/posts/activity-7072349356980404224-6YNE?utm_source=share&utm_medium=member_desktop), in this part, we will explore the process of building an ML training pipeline using ZenML, a machine learning operations (MLOps) framework, and the Heart Disease UCI dataset available on Kaggle. ZenML provides a streamlined approach to construct efficient and scalable ML pipelines, allowing us to seamlessly preprocess the data, perform feature selection, train and evaluate ML models, and deploy them for real-world use. By leveraging the rich features of the Heart Disease UCI dataset, such as age, blood pressure, cholesterol levels, and more, we can build a robust pipeline using ZenML to develop accurate predictive models for heart disease classification. Join me on this journey as we dive into each step of the pipeline, harnessing the power of ZenML to construct a reliable ML training pipeline for heart disease classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install zenml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"zenml[server]\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kickstart our data adventure like a supercharged ML engineer....shall we? sure? ... ready to unravel the secrets hidden within! We'll grab our data and dive headfirst into the exciting world of exploration, armed with curiosity and a touch of mischief (ok, may be not exactly like this, but I just couldnt control excitement, its Saturday Today, and next to me is my favorite expresso, what else would you expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download -d ketangangal/heart-disease-dataset-uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip_file(zip_path, extract_dir):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "unzip_file('./heart-disease-dataset-uci.zip', './dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "heart_dis_data = pd.read_csv('./dataset/HeartDiseaseTrain-Test.csv')\n",
    "heart_dis_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, lets now do EDA ... ( its Exploratory Data Analysis , yes yes, me too, I learned this fairly recently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the dataset\n",
    "print(\"Shape of the dataset:\", heart_dis_data.shape)\n",
    "print(\"\\nInformation about the dataset:\")\n",
    "heart_dis_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape..? wait what? why we need to see shape of data?\n",
    "\n",
    "When, we do data.shape, it will display the number of rows and columns in the DataFrame. The output will be in the format (num_rows, num_columns). This provides a quick way to understand the size and structure of the dataset\n",
    "\n",
    "Thats good, but.. why do we care?\n",
    "\n",
    "Here is how I understand it, shape provides a quick overview of the dataset's size and structure. It helps us understand the number of rows (Note :: this is nothing but instances) and columns (which is..?..yes features, very very important to note) present in the dataset. This information is crucial for understanding the data's complexity, determining the feasibility of applying certain algorithms, and estimating the computational resources required for training models.\n",
    "\n",
    "See, knew before how important this simple method call is ?  same here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the summary statistics\n",
    "print(\"\\nSummary statistics of numerical columns:\")\n",
    "print(heart_dis_data.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find missing values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
